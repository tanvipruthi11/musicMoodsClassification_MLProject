{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Synthetic_Data_Generation.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyO+EswpqaJsvJpZ9usJEmWy"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Generative Adversarial Network (GAN)\n","\n","Generative modeling is an unsupervisd learning task thta involves automatically discovering and learning the regularities in patterns in input data. GAN is a way of training the model by framing the problem as a supervised learning problem with two suib-models: generator and discriminator.\n","\n","Generator is training to generate new data points and discriminator tries to classify them as real or generated (fake) data. If the discriminator identifies the fake data, the generator is penalized, otherwise, the discriminator is penalized."],"metadata":{"id":"8z01nHUVdjFH"}},{"cell_type":"markdown","source":["Imports"],"metadata":{"id":"s6PN28SiewO4"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"1R-boum3Mx4C"},"outputs":[],"source":["import sklearn.cluster as cluster\n","from sklearn.preprocessing import PowerTransformer\n","\n","import tensorflow as tf\n","from tensorflow.keras.layers import Input, Dense, Dropout\n","from tensorflow.keras import Model\n","from tensorflow.keras.optimizers import Adam\n","\n","import matplotlib.pyplot as plt\n","\n","import pandas as pd\n","import numpy as np\n","import io\n","import os"]},{"cell_type":"markdown","source":["### load_data_from_files\n","\n","This loads the data from the file. We pass the parameter of the file name.\n","It converts the label column into simple category code - 0 in our case, as we have split our data based on the emotion label.\n","\n","It then converts the numbers in gaussian for the training.\n","\n","*We split our data into separate files, as we were not able to convert the label back from gaussian. We still have to test the difference it makes when generating the data by where all the data is passed and when we split the data by emotion.*"],"metadata":{"id":"KQTorJXne5Nx"}},{"cell_type":"code","source":["def load_data_from_files(input_file_name):\n","  # Read the original data and have it preprocessed\n","  data = pd.read_csv(input_file_name)\n","  data.drop([\"uri\"], axis=1, inplace=True)\n","  data_cols = data.columns\n","\n","  # Convert Label to category numbers\n","  data[\"label\"] = data[\"label\"].astype('category').cat.codes\n","\n","  # Convert Data into gaussian\n","  data[data.columns] = PowerTransformer(method='yeo-johnson', standardize=True, copy=True).fit_transform(data[data.columns])\n","\n","  return data"],"metadata":{"id":"wuG56a8mNBeQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### GAN\n","\n","The class accepts params that can be changed to improve our model\n","\n","The class gets the training data and trains the model for the specified number of iterations (epochs)\n","\n","We save the trained discriminator and generator to generate the data later"],"metadata":{"id":"5qYFLewnf_zi"}},{"cell_type":"code","source":["class GAN():\n","    \n","    def __init__(self, gan_args):\n","        [self.batch_size, lr, self.noise_dim,\n","         self.data_dim, layers_dim] = gan_args\n","\n","        self.generator = Generator(self.batch_size).\\\n","            build_model(input_shape=(self.noise_dim,), dim=layers_dim, data_dim=self.data_dim)\n","\n","        self.discriminator = Discriminator(self.batch_size).\\\n","            build_model(input_shape=(self.data_dim,), dim=layers_dim)\n","\n","        optimizer = Adam(lr, 0.5)\n","\n","        # Build and compile the discriminator\n","        self.discriminator.compile(loss='binary_crossentropy',\n","                                   optimizer=optimizer,\n","                                   metrics=['accuracy'])\n","\n","        # The generator takes noise as input and generates imgs\n","        z = Input(shape=(self.noise_dim,))\n","        record = self.generator(z)\n","\n","        # For the combined model we will only train the generator\n","        self.discriminator.trainable = False\n","\n","        # The discriminator takes generated data as input and determines validity\n","        validity = self.discriminator(record)\n","\n","        self.combined = Model(z, validity)\n","        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n","\n","    def get_data_batch(self, train, batch_size, seed=0):\n","\n","        start_i = (batch_size * seed) % len(train)\n","        stop_i = start_i + batch_size\n","        shuffle_seed = (batch_size * seed) // len(train)\n","        np.random.seed(shuffle_seed)\n","        train_ix = np.random.choice(list(train.index), replace=False, size=len(train))\n","        train_ix = list(train_ix) + list(train_ix) \n","        x = train.loc[train_ix[start_i: stop_i]].values\n","        return np.reshape(x, (batch_size, -1))\n","        \n","    def train(self, data, train_arguments):\n","        [cache_prefix, epochs, sample_interval] = train_arguments\n","        \n","        data_cols = data.columns\n","\n","        valid = np.ones((self.batch_size, 1))\n","        fake = np.zeros((self.batch_size, 1))\n","\n","        for epoch in range(epochs):    \n","            #  Train Discriminator\n","            batch_data = self.get_data_batch(data, self.batch_size)\n","            noise = tf.random.normal((self.batch_size, self.noise_dim))\n","\n","            # Generate a batch of new data\n","            gen_data = self.generator.predict(noise)\n","    \n","            # Train the discriminator\n","            d_loss_real = self.discriminator.train_on_batch(batch_data, valid)\n","            d_loss_fake = self.discriminator.train_on_batch(gen_data, fake)\n","            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n","\n","            #  Train Generator\n","            noise = tf.random.normal((self.batch_size, self.noise_dim))\n","            # Train the generator (to have the discriminator label samples as valid)\n","            g_loss = self.combined.train_on_batch(noise, valid)\n","    \n","            # Plot the progress\n","            print(\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100 * d_loss[1], g_loss))\n","      \n","      # Save generator and discriminator weights\n","        model_checkpoint_base_name = 'model/' + cache_prefix + '_{}_model_weights.h5'\n","        self.generator.save_weights(model_checkpoint_base_name.format('generator', epoch))\n","        self.discriminator.save_weights(model_checkpoint_base_name.format('discriminator', epoch))\n","\n","    def save(self, path, name):\n","        assert os.path.isdir(path) == True, \\\n","            \"valid path needed from colab\"\n","        model_path = os.path.join(path, name)\n","        self.generator.save_weights(model_path)  # Load the generator\n","        return\n","    \n","    def load(self, path):\n","        assert os.path.isdir(path) == True, \\\n","            \"valid path needed from colab\"\n","        self.generator = Generator(self.batch_size)\n","        self.generator = self.generator.load_weights(path)\n","        return self.generator\n","    \n","class Generator():\n","    def __init__(self, batch_size):\n","        self.batch_size=batch_size\n","        \n","    def build_model(self, input_shape, dim, data_dim):\n","        input= Input(shape=input_shape, batch_size=self.batch_size)\n","        x = Dense(dim, activation='relu')(input)\n","        x = Dense(dim * 2, activation='relu')(x)\n","        x = Dense(dim * 4, activation='relu')(x)\n","        x = Dense(data_dim)(x)\n","        return Model(inputs=input, outputs=x)\n","\n","class Discriminator():\n","    def __init__(self,batch_size):\n","        self.batch_size=batch_size\n","    \n","    def build_model(self, input_shape, dim):\n","        input = Input(shape=input_shape, batch_size=self.batch_size)\n","        x = Dense(dim * 4, activation='relu')(input)\n","        x = Dropout(0.1)(x)\n","        x = Dense(dim * 2, activation='relu')(x)\n","        x = Dropout(0.1)(x)\n","        x = Dense(dim, activation='relu')(x)\n","        x = Dense(1, activation='sigmoid')(x)\n","        return Model(inputs=input, outputs=x)"],"metadata":{"id":"1yePiVvaNS2H"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### train_gan_model\n","\n","This method passes the arguments required by GAN. It runs the model and returns the synthesizer\n","\n","We are using epoch to be 1000, as while testing, I noticed that the accuracy was sometimes reached above 95% when the number of iterations increased significantly."],"metadata":{"id":"jxvpPI65gg2w"}},{"cell_type":"code","source":["def train_gan_model(train_data, emotion):\n","  # GAN training params\n","  noise_dim = 32\n","  dim = 128\n","  batch_size = 64\n","\n","  log_step = 100\n","  epochs = 1000+1\n","  learning_rate = 5e-4\n","  models_dir = './cache'\n","\n","  gan_args = [batch_size, learning_rate, noise_dim, train_data.shape[1], dim]\n","  train_args = [emotion, epochs, log_step]\n","  model = GAN\n","  synthesizer = model(gan_args)\n","  synthesizer.train(train_data, train_args)\n","  return synthesizer"],"metadata":{"id":"KASbSTlgNcSi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### generate_synthetic_data\n","\n","This generates the synthetic data for the specified emotion and given number of data points"],"metadata":{"id":"T7i9W7bAgu03"}},{"cell_type":"code","source":["def generate_synthetic_data(generator, emotion, data_points=500):\n","  np.random.seed(17)\n","  z = np.random.normal(size=(data_points, 32))\n","\n","  # Generating Synthetic Data\n","  generator.load_weights( 'model/' + emotion + '_generator_model_weights.h5')\n","\n","  synthetic_z = generator.predict(z)\n","  synthetic_data = pd.DataFrame(synthetic_z, columns=train_data.columns)\n","  synthetic_data.to_csv('output/synthetic_' + emotion + '.csv')\n"],"metadata":{"id":"sC9ETW21Onx3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Simulator\n","\n","This is where we call the above methods for all our emotion data"],"metadata":{"id":"2uF1c4K6g8Gu"}},{"cell_type":"code","source":["# Create Directories to save Model and Synthetic data\n","!mkdir model\n","!mkdir output\n","\n","emotions = ['aggressive', 'calm', 'chill', 'dark', 'energetic', 'relaxing']\n","\n","for emotion in emotions:\n","  train_data = load_data_from_files(emotion + '.csv')\n","  synthesizer = train_gan_model(train_data, emotion)\n","  generator = synthesizer.generator\n","  generator.summary()\n","  synthesizer.discriminator.summary()\n","  generate_synthetic_data(generator, emotion, data_points=600)"],"metadata":{"id":"7Tx1RKSETgr5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Further Steps\n","\n","I will be testing this to generate data for all the labels combined and then separately and analyse the difference in the correlation of the data.\n","\n","The data currently generated by this was not very useful.\n","I have to analyse the difference it makes when the data is not converted to gaussian and come up with a better model"],"metadata":{"id":"obvcX7kJhdYM"}}]}